# -*- coding: utf-8 -*-
"""MinorProject_B21EE038.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S1WGJupY9Ku4kwmmdp8afslskCkHdgU-
"""

# Import libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA, FastICA
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, pairwise_distances
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.neighbors import kneighbors_graph
from sklearn.cluster import DBSCAN
import plotly.graph_objs as go
import plotly.offline as pyoff
from sklearn.decomposition import FastICA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

#pd.options.display.max_rows = 40000
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx'
data = pd.read_excel(url)
print(data.head())

print(data.nunique())
print(data.isnull().sum())

data.UnitPrice.value_counts(normalize=False)

data['expensive'] = data['UnitPrice'].apply(lambda x: 1 if x > 10 else 0)
data['intermediate'] = data['UnitPrice'].apply(lambda x: 1 if (x > 5 and x<10) else 0)
print(data.expensive.value_counts(normalize = False))
print(data.intermediate.value_counts(normalize = False))

print(data.describe())

from datetime import datetime
data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])                                           #Here, to check no of customers on weekdays and weekends, we create a new 
data['Day'] = data['InvoiceDate'].apply(lambda x: datetime.strftime(x, '%A'))                       #column 'Day'.

import matplotlib.pyplot as plt       
fig = plt.figure(figsize=(12,4))     
data.Day.value_counts().plot(kind="bar",alpha=1,color = 'red')        #This shows that no shopping happened on Saturday
print(data.Day.value_counts())

data['TotalCost'] = data['Quantity']*data['UnitPrice']
data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])
weekday_total = data.groupby(data['InvoiceDate'].dt.strftime('%A'))['TotalCost'].sum()
weekday_total.plot(kind='bar', figsize=(10,5))
plt.title('Total cost of products for each day')
plt.xlabel('Day')
plt.ylabel('Total cost of products')
plt.show()

data['InvoiceYearMonth'] = data['InvoiceDate'].map(lambda date: 100*date.year + date.month)
TotalCost = data.groupby(['InvoiceYearMonth'])['TotalCost'].sum().reset_index()
#X and Y axis inputs for Plotly graph. We use Scatter for line graphs
plot_data = [
    go.Scatter(
        x=TotalCost['InvoiceYearMonth'],
        y=TotalCost['TotalCost'],
    )
]

plot_layout = go.Layout(
        xaxis={"type": "category"},
        title='Montly Revenue'
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

# Remove missing values
data = data.dropna()

# Encode categorical variables
encoder = LabelEncoder()
data['Country'] = encoder.fit_transform(data['Country'])

# Extract features and target variable
X = data.drop(['CustomerID', 'InvoiceNo', 'StockCode', 'Description', 'InvoiceDate', 'Country','expensive','intermediate','Day'], axis=1) #i.e applied UnitPrice,Quantity
y = data['CustomerID']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Dimensionality reduction
# PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Get the explained variance ratio
variance_ratio = pca.explained_variance_ratio_

# Calculate the total variance retained
total_variance_retained = np.sum(variance_ratio)

# Print the results
print("Explained variance ratio: ", variance_ratio)
print("Total variance retained: ", total_variance_retained)
# # ICA
# ica = FastICA(n_components=2)
# X_ica = ica.fit_transform(X_scaled)
# # Get the explained variance
# variance = ica.explained_variance_ratio_

# # Calculate the total variance retained
# total_variance_retained = np.sum(variance)

# # Print the results
# print("Explained variance: ", variance)
# print("Total variance retained: ", total_variance_retained)

# Clustering
# K-Means
kmeans = KMeans(n_clusters=4, random_state=42,n_init = 1)
kmeans.fit(X_pca)
y_pred_pca_km = kmeans.predict(X_pca)
# Performance Evaluation
# Silhouette Score
print('Silhouette Score for PCA K-Means:', silhouette_score(X_pca, kmeans.labels_))

# assuming X is the preprocessed data and y is the target variable
lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(X_scaled, y)
kmeans = KMeans(n_clusters=3, random_state=42,n_init = 1)
kmeans.fit(X_lda)
y_pred_lda_km = kmeans.predict(X_lda)

# Plot the results
plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y_pred_lda_km)
plt.xlabel('LDA Component 1')
plt.ylabel('LDA Component 2')
plt.title('LDA Results')
plt.show()

# Plotting clusters
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred_pca_km)
plt.title('Clusters formed after applying PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

# Elbow Method
# K-Means with PCA
distortions = []
for i in range(1, 11):
    km = KMeans(n_clusters=i, random_state=42,n_init = 1)
    km.fit(X_pca)
    distortions.append(km.inertia_)

plt.plot(range(1, 11), distortions, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Distortion')
plt.show()

# KNN Classification
# KNN with PCA
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_pca, y)
y_pred_pca_knn = knn.predict(X_pca)

# Creating KNN graph
knn_graph = kneighbors_graph(X, n_neighbors=100)

# Plotting KNN graph
plt.spy(knn_graph, markersize=1)
plt.title('KNN graph')
plt.xlabel('Sample index')
plt.ylabel('Sample index')
plt.show()

# Splitting data into training and testing sets
 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # # Random Forest
# # rf = RandomForestClassifier(n_estimators=100, random_state=42)
# # rf.fit(X_train, y_train)
# # print('Accuracy of Random Forest:', rf.score(X_test, y_test))

# Support Vector Machines
svm = SVC(kernel='rbf', C=10, gamma=0.1)
svm.fit(X_train, y_train)
print('Accuracy of SVM:', svm.score(X_test, y_test))